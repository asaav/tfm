\section*{Resumen}
\addcontentsline{toc}{section}{Resumen}
\markright{Resumen}

En este trabajo de fin de máster se ha abordado, desde cero, el análisis, diseño e implementación de un proyecto de visión por computador utilizando \textit{deep learning}, creando un dataset propio a partir de vídeos obtenidos en un entorno real. El objetivo era, utilizando vídeos de partidos de voleibol, tomados mediante una cámara con vista cenital sobre el campo, crear un dataset con la suficiente calidad como para entrenar un modelo de red neuronal capaz de detectar el balón desde cero en los frames. Esto es útil de cara a análisis deportivo o \textit{scouting}, dando la posibilidad de analizar trayectorias, golpeos y velocidades del balón.

Las herramientas que se han utilizado en el proyecto son Python como lenguaje de desarrollo, y las librerías OpenCV y Tensorflow, las cuales nos permiten aplicar técnicas de visión por computador y entrenar modelos de redes neuronales, respectivamente. La segunda librería es especialmente importante para nuestro cometido ya que es la que nos va a dar todo el conjunto de herramientas para obtener un modelo \textit{deep learning} que cumpla la tarea que hemos planteado.

Las imágenes se obtendrán de una cámara modelo AXIS P1365-E Mk II, instalada en el pabellón Eduardo Linares de Molina de Segura, donde entrena el club de voleibol femenino Molina Voley, que se ha prestado a proporcionarnos imágenes de partidos. La cámara nos dará imágenes en vista cenital del campo en vídeos de 1080p y 30 FPS.

De los frames de estos vídeos obtendremos pequeñas imágenes de ejemplo con y sin balón, y utilizaremos estas imágenes para alimentar un modelo de red neuronal completamente convolucional (FCN), cuyas ventajas sobre un modelo convolucional convencional son principalmente que acepta imágenes de cualquier tamaño, lo cual significa que podremos utilizar el modelo una vez entrenado con estas imágenes de tamaño reducido para predecir frames en su tamaño completo. Un problema a considerar en este aspecto del proyecto es la dificultad de obtener ejemplos de forma automática de los vídeos, ya que, aunque se utilizará el trabajo de fin de grado anterior a este proyecto, el cual tenía problemas en los solapamientos entre jugadoras y balón. Para sortearlos, no queda otra opción que completar el resto del dataset con imágenes extraídas de forma manual y depurar finalmente los datos.

Otra técnica a aplicar de cara al entrenamiento del modelo será la de \textit{data augmentation}, que permitirá una mayor regularización del modelo, aplicando transformaciones aleatorias a las imágenes de entrenamiento del dataset. Estas modificaciones deberían ayudar a que el modelo extraiga las \textit{features} más relevantes de cada imagen, además de lograr una mejor regularización del modelo.

Tras el entrenamiento del modelo, se obtuvo una precisión de hasta el 99\% sobre el conjunto de test tras hacer una selección de hiperparámetros, aunque se apreciaba un problema de falsos positivos una vez aplicado el detector a frames completos. Para solucionar este problema, se aplicó una técnica llamada Hard Negative Mining. Esta técnica consiste en utilizar los errores del modelo en entornos reales (en nuestro caso, los partidos) e introducirlos como ejemplos en el dataset con el que hemos entrenado dicho modelo. El resultado deseado es que el modelo aprenda de sus errores y vaya depurándolos como parte del proceso de aprendizaje, mejorando su salida.

Utilizando la técnica de Hard Negative Mining se logró un modelo suficientemente bueno como para poder hacer seguimiento del balón, si bien este seguimiento no es perfecto debido a las dificultades intrínsecas de algunos frames, algunas veces causadas por la forma borrosa del balón, otras veces causadas por la similitud entre el balón y otros objetos de la escena en color o forma. Para salvar este tipo de errores no queda otra opción que la de implementar alguna forma por rudimentaria que sea de coherencia temporal ya que ha comprobado que la detección desde cero en todos los frames no es posible en estas circunstancias.

En suma, el proyecto ha llegado a una conclusión moderadamente satisfactoria pese a los obstáculos con los que nos hemos ido encontrando durante el camino, durante todas las fases de este. Una posible alternativa de cara a una mejora futura sería utilizar un sistema de etiquetado distinto al actual, que es la asignación de una clase a la imagen en global, y en su lugar, etiquetar cada píxel de esta. Este cambio de paradigma haría de este problema uno de segmentación visual, y dado que una FCN es una arquitectura típicamente usada para este tipo de problemas, no sería necesaria una gran modificación. Sin embargo, hay que tener en cuenta que el etiquetado de tal cantidad de datos es una tarea de una envergadura tal que quedaría fuera del alcance del trabajo actual.


\newpage

\begin{otherlanguage}{english}
    
\section*{Extended Abstract}
\addcontentsline{toc}{section}{Extended Abstract}
\markright{Extended Abstract}

In this end of masters’ thesis, we covered the analysis, design and implementation of a deep learning-based computer vision solution from scratch, building our own dataset using real images from a camera. This camera, which is installed in the Eduardo Linares high school’s sports hall, shows a top-down view of a volleyball court. The videos from the camera have 1920x1080 resolution at 30 frames per second.
Therefore, our main purpose was to make this dataset and, as a side objective, to train a neural network model using it that was capable of detecting a ball in a volleyball match in every frame of the videos that we will use, without any form of temporal coherence mechanism. This model would be useful for sport analysis and scouting purposes while maintaining a low cost of implementation.

Computer vision is a field of knowledge that seeks to make computers able to understand images and videos at a human level. Although it works with computers, it is not just a matter of research for computer scientists: there are psychologists, opticians and several other scientific researchers actively searching for new algorithms and techniques to make computer vision as close to that of humans as possible. In its beginnings, the difficulty of computer vision was often underestimated, but nowadays it is considered as one of the main barriers in the way to a true AI. Proof of that underestimation is that in 1966, an MIT student was assigned a summer project which consisted in attaching a camera to a computer and making it describe in natural language what it saw on that camera. Later, the world of computer research began to realize the true difficulty inherent to the issue at hand.

Over the years, a lot of algorithms, techniques and models have been developed, from simple contours and border detections to visual segmentation of a scene in real time. The most important advancement so far in this discipline has been the development of ANNs (Artificial Neural Networks), which have proven to be very successful in a different array of domains. This approach to computer vision has been the dominant trend from the early 2010s until today because of the surge of Internet and the large, labelled datasets that have come with it along with the improvement of computation capabilities of modern computers.

Neural networks are not a new concept, in fact, they were proposed back in 1943 by Warren McCulloch and Walter Pitts in an article. ANNs have gone through two research booms until nowadays, that we see the third and maybe final wave of progress in this kind of models.

Currently, there are plenty of computer vision solutions to a wide range of problems. In sports, for example, we can find Hawk-Eye, a popular computer vision system used mainly in tennis but also found in some other sports. In tennis, Hawk-Eye is used by referees and players to help determine if a ball was out of bounds or not. There are also solutions oriented to gather statistics useful for scouting purposes and coaching, like SportsVU, which uses images from multiple cameras to present statistics such as heat maps, possession maps, passing maps, etc. This system can be used in several sports, like basketball, football, rugby and some others. Lastly, we have solutions oriented to the audience like Intel 360 Replay. Again, this technology processes data from many cameras around the playing field (38 in the case of football) to provide 3D reconstructed replays of actions of the game.

Using deep learning there have also been several solutions, such as DeepBall, which was an object detector specialized for ball detection in long shot videos, using a fully convolutional network (FCN) and trained with the ISSIA-CNR Soccer dataset. The method achieved an 87,7\% accuracy using data augmentation. The shortcomings were ball occlusions and small objects very similar to the ball such as players’ boots, goalkeepers’ gloves or some bits of the billboards.

About the goals of this project, the main purpose was, as has been discussed, to build a dataset from images with a top down perspective to a volleyball court from a camera installed in the Eduardo Linares high schools’ sports hall. As a second objective, we sought to train a neural network capable of determining the presence of the ball in an image with enough accuracy to be able to track the ball in the video. The neural network will be an FCN because it has the advantage that it can process images of any given size, although our training images will be 56x56.
The tools we have used to develop this project are: 

\begin{itemize}
    \item Python. Being one of the most used programming languages for machine learning and scientific data processing, as well as a very easy language to develop in, Python is a sure choice when dealing with deep learning projects such as this one. Python also has lots of libraries that will help us in our task, like TensorFlow or OpenCV, among others.
    \item TensorFlow. This is maybe the most important tool in this project. TensorFlow, alongside with Keras, provided us with the API to design and train several neural network architectures.
    \item OpenCV. This was a very useful library, that helped us in the computer vision related tasks as well as in acquiring some of the examples in our dataset.
    \item Git. One of the most important aspects in software development of any kind is version control software (VCS), such as Git, which is the most popular VCS now because of its simplicity and power. Using it was useful for feedback purposes from the tutors as well as keeping a historic changelog.
\end{itemize}

Before creating our dataset, we checked if a generalist approach would be enough to solve the problem in hand. For this purpose, we used a model implemented in OpenCV, which is GOTURN, short for Generic Object Tracking Using Regression Networks. This method uses an underlying neural network which is trained using a ALOV300++ dataset, a publicly available dataset containing images for several categories of object such as people or balls. To test this model, we just needed to download the trained weights from Internet and use them in our software. The results of this model were not very good, since the tracking of our ball failed within the first seconds of initialising it, and attached commonly to a line of the court. Because of this, we concluded that the generalist approach was not enough for this problem, and we would need a solution more adjusted to our purpose.

As to the making of the dataset, it was conformed of positive and negative examples. To get all the example images, there were both an automatic and a manual part. As for the automatic part, my end-of-degree project was used, where a ball detector was developed using background subtraction. Nevertheless, we found the same limitations that were present in that project, that were mainly the overlapping between players and the ball, and some instances where players were identified as the ball. Because of these limitations, the positive examples would be skewed towards instances where the ball is isolated without players near it. To acquire negative examples, we just picked a random position from the frame that did not contain the ball.

Once done with this process, our dataset had 9014 images, of which 6886 were negatives and 2128 were positives. As we discussed, the system had some problems in detection, so these images had to be manually checked for errors. After doing this, the dataset had 8023 images, with 6877 negatives and 1146 positives.

With a clean dataset, we needed to compensate for the shortcomings of the automatic approach which were mainly overlaps between players and ball. On the negative examples, we needed some examples of objects other than the background to help the model in the learning process. Since we were lacking this kind of examples, we handpicked some more examples from the videos, bringing the total count of examples to 9526 with 6961 negatives and 2565 positives. For the training part, the dataset was split in an 80-10-10\% proportion between training, validation and test, respectively.

The dataset was then stored in our disk using a folder structure which has a folder for each of the train, test and validations subsets, and a subfolder for each class (in this case, 2 as we only have positives and negatives).

Although almost 10000 examples is an arguably high number, it still does not guarantee that the model is going to learn all the features it needs, so we used a technique called data augmentation. This consists in applying a random transformation to each image in the train dataset to amplify the number of images available to the model, helping regularize its results.

As said before, we picked an FCN architecture, for its versatility when it comes to processing images of any size, so once trained in the dataset, our model should be capable of predicting frames of the videos themselves. Using the default parameters, the model gave an accuracy over 97\%.

To adjust the hyperparameters of our model, we first set up early stopping, which reduces training time by stopping it once the model stops improving in a given metric (in our case, validation loss) and also prevents it from overfitting training data.

Since the model tended to fluctuate in the training metrics, we adjusted the learning rate coefficient from a $10^{-3}$ value to $10^{-4}$ which makes the model learn slower but at a more stable pace, hopefully achieving a better solution.

The last parameter that was adjusted was class weight. This parameter was important because of the class imbalance that our dataset had, in order to avoid the model to become very skewed towards the negatives. We raised the class weight for the positive examples to 2.5, a value that would put a similar importance between positive and negative classes, and it showed an accuracy in the test dataset of 99.47\%.

Even though our model was able to identify much better the ball in scenes, there was a good deal of false positives that needed to be addressed when testing the model against the videos. In order to clean those false positives up, we used those error instances as training examples in an iterative process, called Hard Negative Mining.

As said, Hard Negative Mining consists in taking real environment errors in a model and introducing them in the dataset that was used to train said model. This process can be performed over negative and positive examples and can be done any number of times until we are left with a model that satisfies our requirements.

Nevertheless, determining what an error is requires some form of labelling in every frame of the videos in order to know which positives are real and which ones are prediction errors. Labelling every frame in over 20 minutes of videos that we have at the moment would be a very demanding task to perform and is out of the scope of this project. Instead, we took a small sample of 30 images and labelled the position of the ball in each one. Using those labels, we could locate the errors that our model made, by comparing the positives that it detected with said labels, which act as ground truth, and store our model's false positive and false negatives.

There's also one more thing to consider, and it is that we needed to make sure that the examples we picked were not very close to each other in order to avoid examples that look exactly the same ending up in different subsets of the dataset. This means we must check the position of every patch not just in one of the frames, but in every one of them. Once calculated every error, we select them taking no more than half of the number of images in the dataset and split them in the same 80-10-10\% proportion. The new images are then introduced in the dataset and we delete the same number of images from it that we will introduce, thus creating a new, refined version of it. 

This new version of the dataset is stored in a separate folder of our drive. One aspect to consider in this process is the amount of memory that it would require in a big enough dataset and with a rather big number of iterations. To avoid that, instead of copying the full dataset in every iteration, we use symbolic links to the images of the last iteration and just save as new files the errors that we have detected.

Usually no more than 3 iterations of this process are necessary to achieve a model that is good enough. The prediction errors are for the most part solved, but we still must manage a little amount that occur due to intrinsic difficulty of some frames.

To initialize our tracker, we will first give it a ROI in order to have a human-supervised first position of the ball and then we will keep following that ROI as the ball moves instead of running the model over the full image, which will help us have a better performance.

As to the performance itself, we found the model to be fairly competent although there are many problems in some situations like the ball changing direction or speed suddenly, which makes the window not fast enough to adjust itself. This kind of errors are not recoverable with the approach that we use.

There's also a problem with the framing of the ball, because we need to reconstruct the position of the ball given the reduced image that is the output of the model. One way to truly solve this issue is to use a \textit{deconvolution} in the end of our architecture, thus resizing the output of the model to be the same size of the original frame.

In summary, we ended with a model that was good at performing the task in hand but not very consistent and very prone to some errors that cannot be easily avoided like some cases when the ball is too blurry to recognize and in small objects like players’ shoes.

As future improvements, there are a lot of tasks that would improve this project but cannot have been performed due to their size or time requirements that made them out of scope. Some of these tasks are:

\begin{itemize}
    \item Use of any kind of temporal coherence algorithm like Hidden Markov Models, Kalman filters or RNNs to help fill in the blanks left by the detection errors of our model.
    \item Label the position of the ball in every frame of the videos, thus having a semantic segmentation approach and use a model that instead of predicting the full image, gives a prediction for each pixel of it.
    \item In combination with the previous, we could used some popular and established FCN architecture, like YOLO or R-CNN.
\end{itemize}

As a final improvement and something that is out of reach is the improvement of both the resolution and framerate of the camera, which would help reduce some of the intrinsic errors that we mentioned.

\end{otherlanguage}
\newpage